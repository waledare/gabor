\begin{proof} We begin by noting that 
\begin{align}
  \svnx - \sigma^2(t) & = \sum_{(h,k) \in \Theta_n} (\cnhk\ - \chk)\;g_{h,k}(t)\notag\\
  & \quad -\sum_{(h,k) \not\in \Theta_n} \chk\;g_{h,k}(t),\label{eq:ronep} 
\end{align}
  where 
\begin{align}  
  \cnhk &= \sum_{i =0}^{n-1} \btghki (X_{t_{i+1}} - X_{t_i})^2 \text{ and }\notag\\  
  \chk &= \int^1_0 \btghks \sigma^2(s) \D s. \notag
\end{align}
We tackle the summands in \eqref{eq:ronep} in turn starting with the first one. But first let 
\begin{align}
  M_i := \int_{t_i}^{t_{i+1}} b(s) \D s, \quad \text{and} \quad  S_i := \int_{t_i}^{t_{i+1}} \sigma(s) \D W_s, \notag
\end{align}
and note that since $X_{t_{i+1}} - X_{t_i} = M_i + S_i$, it follows that
\begin{align}
  (X_{t_{i+1}} - X_{t_i})^2 &= M_i^2  
  + 2M_iS_i +   S_i^2.\notag 
\end{align}
So, \eqref{eq:ronep} may be written as 
\begin{align}
  &\svnx - \sv(t) = B_{1,n}(t) + B_{2,n}(t) + B_{3,n}(t) + B_{4,n}(t), \notag
\end{align}
where
\begin{align}
  &B_{1,n}(t) :=  \sumt g_{h,k}(t) \left(\sum_{i=0}^{n-1} \btghki S_i^2  - \chk\right), \notag\\
  &B_{2,n}(t) := 2 \sumt g_{h,k}(t)\left(\sum_{i=0}^{n-1} \btghki S_i M_i \right), \notag\\
  &B_{3,n}(t) := \sumt g_{h,k}(t)\left( \sum_{i=0}^{n-1} \btghki M_i^2 \right), \notag\\
  &B_{4,n}(t) := - \sum_{\substack{(h,k) \not\in \Theta_n }}g_{h,k}(t)\chk. 
  \label{}
\end{align}
We will estimate the summands starting with $B_{4,n}(t)$. Note the following:
\begin{align}
  \sum_{\substack{(h,k) \not\in \Theta_n }}g_{h,k}(t)\chk &=  \sum_{\substack{(h,k) \not\in \Theta_n }}g_{h,k}(t)\int^1_0\sigma^2 (s) { \btghks} \D s \notag\\ 
&\le c\modc{\tg}{1/H_n}\log H_n  + c\modc{\sigma^2}{1/H_n} \log H_n\notag,
\end{align}
where the last  line follows from Lemma \ref{th:fourone} and Lemma \ref{lem:modtg}. It follows from the \holder continuity of $\sigma^2$ that   $\modc{\sigma^2}{1/H_n} \le c H_n^{-\alpha}$. Furthermore, by Lemma \ref{lem:modtg} and the Lipschitz continuity of $g$ we have $\modc{\tghk}{1/H_n} \le c H^{-1}_n$.  So,  
\begin{align}
 B_{4,n}(t) =  O( H_n^{-\alpha}\log H_n).
  \label{eq:B4}
\end{align}
Note the generic use of the constant  $c$. In the sequel, we will use $c$ to denote the amalgamation of various constants resulting from multiple steps. 

We now obtain an estimate for  $B_{3,n}(t)$. Note the following:
\begin{align}
  M_i^2 & = \left(\int_{t_i}^{t_{i+1}} b(s) \D s\right)^2 \notag\\
  & \le ( (\sup_{s \in [0,1]} b_s) n^{-1})^2. 
  \label{eq:m}
\end{align}
 Now since  $g_{h,k}$ and \tghk  are bounded independently of $h$ and $k$, and $n\Delta_n = 1$, it is almost surely the case that 
\begin{align}
  B_{3,n}(t) = O( H_n \Delta_n).
  \label{}
\end{align}
  Now consider the following, 
\begin{align}
  S_i & = \int_{t_i}^{t_{i+1}} \sigma(s) \D W_s\notag \\
  & = O (\sqrt{n^{-1} \log(n)}),\label{eq:rthree}
\end{align}
for sufficiently large $n$ by a simple corollary to \levy's modulus of continuity theorem \citep[Theorem 10.32]{Rogers1994a}. Hence
\begin{align}
  M_i S_i = O(\sqrt{n^{-3} \log(n)}).\label{eq:ms}
\end{align}
Since  $g_{h,k}$ and \tghk  are bounded independently of $h$ and $k$, and $n\Delta_n = 1$, we have
\begin{align}
  B_{2,n}(t) = O( H_n \sqrt{n^{-1} \log(n)}).
  \label{}
\end{align}
Now we tackle the final piece $B_{1,n}(t)$. Let 
\begin{align}
  A  := \sum_{i=0}^{n-1}\btghki S_i^2 -\int_{0}^{1} \sigma^2(s) \btghks \D s.
\end{align}
We will first obtain an upper bound for $A$; we proceed by adding and subtracting $\sumin\int_{t_i}^{t_{i+1}}\btghki \sigma^2(s)  \D s$  from  $A$ to yield: 
\begin{align}
  A & = \sumin\btghki \left(S_i^2 -\int_{t_i}^{t_{i+1}} \sigma^2(s) \D s\right)\notag \\
  &\quad +  \sumin\left(\int_{t_i}^{t_{i+1}} \sigma^2(s) \{\btghki - \btghks\} \D s\right) \notag\\
  &=:A_{1} + A_{2} \notag.
  \label{}
\end{align}
We obtain estimates in turn for the summands. By linearity of expectation 
\begin{align}
  A_{2} &= \sumin \int_{t_i}^{t_{i+1}} \sigma^2(s)  \{\btghki - \btghks\} \D s\notag\\
  &\le c\modc{\tghk}{\Delta_n},\notag
  \label{}
\end{align}
where $\modc{\tghk}{\Delta_n}$ is the modulus of continuity of $\tghk$ on an interval of length $\Delta_n$. By  Lemma \eqref{lem:modtg} and the Lipschitz continuity of $g$ we have, 
\begin{align}
  A_{2} \le c\modc{g}{\Delta_n} \le c \Delta_n.\notag 
  \label{}
\end{align}

Now, we obtain an estimate for $A_{1}$. First, let 
\begin{align}
  T_c = \inf \{ t >0 : \sigma^2(t) > c \} \wedge 1\notag.
  \label{}
\end{align}
By the regularity assumption on $\sigma$, $\p(T_c < 1)$ can be made arbitrarily small by taking $c$ sufficiently large.  
Now, let $D_i: \Omega\times[0,1] \to \real$ for $i = 0, \cdots, n-1$ be defined as follows:  
\begin{align}
&D_{i}(t) := \btghki\left(\int_{t_i}^{t} \sigma(u)\D W_u\right)\mathbbm{1}_{(t_i, t_{i+1} \wedge T_c]}(t).
  \label{eq:Di}\\
&D_0(0) := 0.
\end{align}
So, $D_i(t)$ is 0 on $[0,1]$ except when $t$ is in $(t_i, t_{i+1} \wedge T_c]$.


Now, using the integration by parts formula for semimartingales, we may write
\begin{align}
  \left(\int^{t_{i+1} \wedge T_c}_{t_i} \sigma(s) d W_s\right)^2  & - \int^{t_{i+ 1}\wedge T_c}_{t_i} \sigma^2(s) \D_s \notag \\ &= 2\int_{t_i}^{t_{i+1}\wedge T_c }\left(\int_{t_i}^{s} \sigma(u)\D W_u\right)\sigma(s) \D W_s\notag
  \label{}
\end{align}
so that 
\begin{align}
  \e(\vert A_1^{T_c} \vert )  &= 2\,\e\left[\sumin\int_{t_i}^{t_{i+1} \wedge T_c}\btghki\left(\int_{t_i}^{s} \sigma(u)\D W_u\right)\sigma(s) \D W_s\right ]\notag \\
  & \le 2\,\e\left[\left\vert\int_0^{1 \wedge T_c}\sumin D_i(s) \sigma(s) \D W_s\right\vert \right].\notag 
  \label{}
\end{align}
Now using the fact that $\int_0^{t \wedge T_c}\sumin D_i(s) \sigma(s) \D W_s$ is a  martingale, we may make an appeal to the Burkholder-Davis-Gundy (BDG) inequality \citep[Theorem 10.36][]{He1992} to yield:
\begin{align}
  \e(\vert A_1^{T_c} \vert) & \le c\e\left[\left\vert\int_0^{1\wedge T_c}\left(\sumin D_i(s) \sigma(s)\right)^2 \D s\right\vert^{1/2} \right]\notag \\
  & \le c\e\left[\left\vert\int_0^{1 \wedge T_c}\sumin \{D_i(s) \sigma(s)\}^2 \D s\right\vert^{1/2} \right]\notag,
  \notag
  \label{}
\end{align}
where the last line follows because $D_i (s) D_j(s) = 0$ whenever $i \not= j$. Now if  we define $D^*_i := \sup_{t_i <s \le t_{i+1} \wedge T_c} D_i(s)$, and use the fact that $\sigma^2$ is less than  $c$ before $T_c$ then
\begin{align}
  \e(\vert A_1^{T_c}\vert ) & \le c\e\left[\left\vert\sumin \Delta_n(D_i^*)^2 \right\vert^{1/2} \right] \notag\\
  & \le c\e\left[\left\vert\sumin \Delta_n(D_i^*)^2 \right\vert \right] \notag\\
  & \le c\Delta_n\sumin\e[(D_i^*)^2]   
  \label{eq:ra1}
\end{align}
where $c$ is a generic constant representing the bound on $\sigma^2$ and the BDG constant.  Note  from the definition of  $D_i$  that it  is itself a martingale, so we may bound  $D^*_i$  with yet another application of the BDG inequality. That is
\begin{align}
  \e((D^*_i)^2) &\le \e\left(\int^{t_{i+1} \wedge T_c}_{t_i} \sigma^2(s) \D s\right) \notag\\
  &\le c \Delta_n. 
  \label{}
\end{align}
Hence, given any $\varepsilon > 0$,  $\p( \sup_{t \in [0,1]} \vert B_{1,n}(t) \vert > \varepsilon)  = O(H_n n^{-1}) + \p(T_c < 1)$.

Collecting the estimates for $B_{j,n}(t)$ for $j =1,\cdots,4$, it is easily seen that $\vert \svnx - \sigma^2(t) \vert$ tends to zero in probability uniformly  for all $t \in \domain$. 
\end{proof}

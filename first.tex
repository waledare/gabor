\documentclass[a4paper, 12pt]{article}
\usepackage{wdmisc}
\usepackage{commath}
%\usepackage{mathtools}
\newcommand{\wmi}{\ensuremath{W_{b_m}\left(\frac{x-Y_{t_i}}{b_m}\right) }\xspace}
\newcommand{\wmj}{\ensuremath{W_{b_m}(({x-Y_{t_i}})/{b_m}) }\xspace}
\newcommand{\kmj}{\ensuremath{K(({x-X_{t_i}})/{b_m}) }\xspace}
\newcommand{\kmi}{\ensuremath{K\left(\frac{X_{t_i} - x}{b_m}\right) }\xspace}
\newcommand{\Xtm}{\ensuremath{X_{t-}}\xspace}
\newcommand{\sbm}{\ensuremath{W}\xspace}
\newcommand{\etn}{\ensuremath{\varepsilon_i^{T_n}}\xspace}
\newcommand{\Wbn}{\ensuremath{W_{b_n}}\xspace}
\newcommand{\ms}{\ensuremath{m^*}\xspace}
\newcommand{\Rs}{\ensuremath{R^*}\xspace}
\newcommand{\lw}{\ensuremath{{L}^W_n(x, T)}\xspace}
\newcommand{\lk}{\ensuremath{{L}^K_n(x, T)}\xspace}
\newcommand{\iid}{i.i.d.\xspace}
\newcommand{\ito}{It\^o\xspace}
\newcommand{\lbm}{\ensuremath{(\triangle_m\ln(1/\triangle_m))}\xspace}
\newcommand{\lbn}{\ensuremath{(\triangle_n\ln(1/\triangle_n))}\xspace}
%\counterwithin*{equation}{chapter}
%\sisetup{output-exponent-marker=\ensuremath{\mathrm{E}}}
\renewcommand{\i}{\mathrm{i}} 
\newcommand{\sumn}{\ensuremath{\sum_{k \in \nats}}\xspace}
\newcommand{\sumin}{\ensuremath{\sum_{i = 0}^{n-1}}\xspace}
\newcommand{\sumi}{\ensuremath{\sum_{k \in \ints}}\xspace}
\newcommand{\sumt}{\ensuremath{\sum_{(h,k) \in \Theta_n}}\xspace}
\newcommand{\sumnt}{\ensuremath{\sum_{(h,k) \not\in \Theta_n}}\xspace}
\newcommand{\sv}{\ensuremath{\sigma^2}\xspace}
\newcommand{\bsv}{\ensuremath{\bar{\sigma}^2}\xspace}
\newcommand{\svnt}{\ensuremath{\sigma^2}\xspace}
\newcommand{\svhk}{\ensuremath{\sigma^2_{h,k}}\xspace}
\newcommand{\vh}{\ensuremath{V_h(\phi)}\xspace}
\newcommand{\idp}{\ensuremath{\mu}\xspace}
\newcommand{\svn}[1]{\ensuremath{v_n(#1, t)}\xspace}
\newcommand{\jvn}{\ensuremath{V_n(X, t)}\xspace}
\newcommand{\svnx}{\ensuremath{v_n(X, t)}\xspace}
\newcommand{\jvnf}{\ensuremath{V_n(X^f, t)}\xspace}
\newcommand{\Svn}{\ensuremath{\hat{\Sigma}_n}\xspace}
\newcommand{\svnb}{\ensuremath{\hat{\sigma}_{n,b}^2}\xspace}
\newcommand{\svnN}{\ensuremath{\hat{\sigma}_{t}^2}\xspace}
\newcommand{\hs}{\ensuremath{\mcal{H}}\xspace}
\newcommand{\T}{\ensuremath{\tau}\xspace}
\newcommand{\chk}{\ensuremath{{c}_{h,k}}\xspace}
\newcommand{\cnhk}{\ensuremath{\hat{c}_{h,k}}\xspace}
\newcommand{\ceen}[1]{\ensuremath{\hat{a}_{h,k}}\xspace}
\newcommand{\cee}{\ensuremath{\chk}\xspace}
\newcommand{\dnhk}{\ensuremath{\hat{d}_{h,k}}\xspace}
\newcommand{\anhk}{\ensuremath{\hat{a}_{h,k}}\xspace}
\newcommand{\ivp}{\ensuremath{\sigma}\xspace}
\newcommand{\inner}[2]{\ensuremath{\langle{#1},{#2}\rangle}\xspace}
\newcommand{\hn}[1]{\ensuremath{\vert{#1}\vert_{\calpha}}\xspace}
\newcommand{\ghk}{\ensuremath{g_{h,k}}\xspace}
\newcommand{\tghk}{\ensuremath{\tilde{g}_{h,k}}\xspace}
\newcommand{\btghki}{\ensuremath{\overline{\tilde{g}_{h,k}(t_i)}}\xspace}
\newcommand{\btghks}{\ensuremath{\overline{\tilde{g}_{h,k}(s)}}\xspace}
\newcommand{\tg}{\ensuremath{\tilde{g}}\xspace}
\newcommand{\czero}{\ensuremath{C^0[0,1]}\xspace}
\newcommand{\domain}{\ensuremath{[0,1]}\xspace}
\newcommand{\calpha}{\ensuremath{C^{0,\alpha}[0,1]}\xspace}
\newcommand{\state}{\czero}
\newcommand{\hkints}{\ensuremath{h,k \in \ints}\xspace}
\newcommand{\mise}{integrated mean aquare error\xspace}
\newcommand{\isqb}{integrated square bias\xspace}
\newcommand{\ivar}{integated variance\xspace}
\newcommand{\holder}{H\"older\xspace}
\newcommand{\homo}[1]{\ensuremath{{| #1 |}_{\alpha}}\xspace}
\newcommand{\hono}[1]{\ensuremath{{\Vert #1 \Vert}_{\alpha}}\xspace}
\newcommand{\hball}[2]{\ensuremath{\mcal{H}{(#1},{#2})}\xspace}
\newcommand{\eball}[2]{\ensuremath{\mcal{E}{(#1},{#2})}\xspace}
\newcommand{\uball}[1]{\ensuremath{\mcal{U}{(#1})}\xspace}
\newcommand{\indvol}{I_{ \eball{\alpha}{c}}}
\newcommand{\ind}{I}
%\newcommand{\indvol}{\mathrm{I}_{\{\sigma^2 \in \hball{\alpha}{c}\}}}
\newcommand{\eind}{{E}_{\eball{\alpha}{c}}} 
\newcommand{\varind}{\var_{\eball{\alpha}{c}}} 
\newcommand{\covind}{\cov_{\eball{\alpha}{c}}} 
\newcommand{\equ}[1]{\begin{align}{#1} \notag\end{align}}
\newcommand{\cadlag}{c\`adl\`ag\xspace}
\renewcommand{\D}{\ensuremath{\dif}\xspace}
\newcommand{\levy}{L\'evy\xspace}
\newcommand{\modc}[2]{\ensuremath{{\bar{\omega}}({#1},#2)}\xspace}
\newcommand{\as}{\ensuremath{\mathrm{a.s.}}\xspace}
\newcommand{\ap}{\ensuremath{\mcal{A}}\xspace}
\newcommand{\pme}{\ensuremath{\mu}\xspace}
\newcommand{\lm}{\ensuremath{\nu}\xspace}
\newcommand{\dx}{\ensuremath{\Delta_i X}\xspace}
\newcommand{\djt}{\ensuremath{\Delta_i J^s}\xspace}
\newcommand{\djl}{\ensuremath{\Delta_i J^l}\xspace}
\newcommand{\dxf}{\ensuremath{\Delta_i X^f}\xspace}
\newcommand{\dxc}{\ensuremath{\Delta_i X^c}\xspace}
\newcommand{\indx}{\ensuremath{\ind_{\{(\dx)^2  \le u_n\}}}\xspace}
\newcommand{\indxf}{\ensuremath{\ind_{\{(\Delta_i X^f)^2  \le u_n\}}}\xspace}
\newcommand{\indxff}{\ensuremath{\ind_{\{(\Delta_i X^f)^2  \le 4u_n\}}}\xspace}
\newcommand{\indn}{\ensuremath{\ind_{\{\Delta_i N = 0\}}}\xspace}
\newcommand{\indnc}{\ensuremath{\ind_{\{\Delta_i N \not= 0\}}}\xspace}
\newcommand{\indno}{\ensuremath{\ind_{\{\Delta_1 N \not= 0\}}}\xspace}
\newcommand{\unidisc}{{\ensuremath{\vert x \vert \le 1}\xspace}}
\newcommand{\cunidisc}{{\ensuremath{\vert x \vert > 1}\xspace}}
\newcommand{\indjs}{\ensuremath{I_{\{(\Delta J^s_i)^2 > u_n\}}}\xspace}
\newcommand{\indjst}{\ensuremath{I_{\{\vert\Delta J^s_i\vert \le 2 \sqrt{u_n}\}}}\xspace}
\usepackage[toc,page]{appendix}
\usepackage{epsf}
\usepackage{bbm}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{lscape}
%\usepackage{float}
%\restylefloat{table}
%\floatstyle{plaintop}
\author{Wale Dare and Matthias Fengler}
\title {Global estimation of realized instantaneous volatility in the presence of \levy price jumps.}
\begin{document}
\maketitle
\begin{abstract}
  We propose a non-parametric procedure for estimating the realized spot volatility of a price process described by  an \ito semimartingale with \levy jumps. The procedure integrates the threshold jump elimination technique \citep{Mancini2009}  with the Gabor frame expansion of the realize spot volatility path.  In the case of continuous assets prices, we show that the estimator converges  in the integrated mean squared error sense. In the case of  asset prices with \levy jumps,  we show that the estimator converges in probability  in $L^2([0,T])$. Our analysis assumes  the time interval between price observations tends to zero; as a result, the intended application is the analysis of high frequency financial data.    

\end{abstract}
%\tableofcontents
Volatility estimation using discretely observed asset prices has received a great deal of attention recently, however,  much of that effort has been focused on 
estimating the \emph{integrated} volatility and, to a lesser extent, the \emph{spot} volatility at a given point in time. 
Notable contributions to the literature on volatility estimation include the papers by \cite{Foster1996}, \cite{Fan2008},   \cite{Florens1993}, and  \cite{BN2004}.
In these studies, the object of interest is local in nature: spot volatility at a given point in time or integrated volatility up to a terminal point in time. In contrast,  estimators which aim  to obtain {spot} volatility estimates  for  entire time windows  have received much less coverage. These are the so-called ``global'' spot volatility estimators. These estimators derive their name from   the fact that the objects of interest are not localized. Typically, a global estimator would be a    random elements whose realizations are  elements of some function space.     

There are potential benefits to adopting global estimators of spot volatility. Given a consistent global estimate of spot volatility $\sigma^2$ over an interval $[0,T]$, the  integrated volatility at any point $t$ within $[0,T]$ may be consistently estimated  by integrating $\sigma^2$ over the interval $[0,t]$.  In fact, by the continuous mapping theorem, consistent estimates of continuous transformations of $\sigma^2$ are immediately available. Hence, integrated powers of spot volatility, $\int^t_0\sigma^p_s \D s$, $p > 0$,  the running maximum of spot volatility, $\sigma^*_t := \sup_{s \le t} \vert \sigma_s \vert$, and volatility in  excess of a given threshold, $\sigma^a_t:=  \sigma_t  I_{\{\vert \sigma_t \vert > a\}}$, $a > 0$,  to name just a few, are easily obtained via  the obvious  transformation of the estimated  global spot volatility.   This flexibility is one of the more appealing features  of this class of estimators.

The paper by \cite{GenonCatalot1992} is an early contribution to the global spot volatility estimation literature. Working within the context of continuous asset prices and deterministic spot volatility paths, the authors  estimate  the realized path of spot volatility using wavelet projection methods.  This basic framework  has since  been extended in the paper by  \cite{Hoffmann2012}, where non-deterministic spot volatility paths and prices subject to market microstructure noise contamination were considered.

Other  seminal contributions to the global spot volatility estimation literature  include the paper by \cite{Malliavin2002}, which relied on Fourier methods to obtain spot volatility estimates in the context of continuous prices. The paper proceeds by  first computing estimates of the Fourier coefficients of the realized price path. Using these estimates, the authors  then derive expressions for  the Fourier coefficients of the realized spot volatility path.     

We contribute to the global spot volatility estimation literature by introducing another class of estimators based on frames as opposed to orthonormal basis. Specifically, we employ Gabor frames in our analysis.  The principal contribution of the current paper besides bringing the flexibility of Gabor frames to volatility estimation, is the proposal of global estimators of spot volatility that remain consistent, in a probabilistic sense,  in the presence of price jumps. So, in situations where the assumption of  continuous asset prices is hard to justify, the  estimators proposed in the current work may prove to be most useful.  

The rest of this paper is organized as follows: in Section \ref{sec:model} we introduce notation and make explicit statements of various assumptions regarding the dynamics  of  observed prices. In  Section \ref{sec:gabor} we introduce   Gabor frames and review the basic theory required for our subsequent analysis. In  Section \ref{sec:estimator}, we introduce a preliminary estimator based on the assumption of continuous asset prices. This basic framework is extended  in  Section 4 to account for prices that  may be subject to \levy jumps.     Section \ref{sec:simulation} describes a simulation exercises that lend further support to the theoretical analysis of  previous sections. Section 6 concludes and points the way for further research. 
\input{jumpsgeneral}
\section{ Frames}\label{sec:gabor} 
Frames generalize the notion of orthonormal bases in  Hilbert spaces. If $\{f_k\}_{k \in \nats}$ is a frame for a separable Hilbert space \hs then every vector $f \in \hs$ may be expressed as a linear combination of the frame elements, i.e.
\begin{align}
  f = \sumn c_k f_k.
  \label{eq:framerep}
\end{align}
This is similar to how elements in a Hilbert space may be expressed in terms of orthonormal basis; but unlike orthonormal basis, the representation in \eqref{eq:framerep} need not be unique, and the frame elements need not be orthogonal. Loosely speaking, frames contain redundant elements. The absence of uniqueness in the frame representation is by no means a shortcoming; on the contrary, we are afforded a great deal of flexibility and stability as a result. In fact, given a finite data sample, the estimated basis expansion coefficients are likely to be imprecise. This lack of precision can create significant distortions when using an orthonormal basis. These distortions are somewhat mitigated when using frames because of the built-in redundancy of frame elements. 



Furthermore, if $\{f_k\}_{k \in \nats}$ is a frame for \hs, then surjective, bounded  transformations of $\{f_k\}_{k \in \nats}$  also constitute frames for \hs, e.g. $\{f_k + f_{k+1}\}_{k \in \nats}$ is a frame. So, once we have a frame, we can generate an arbitrary number of them very easily. We may then obtain estimates using each frame and compare results. If our results using the different frames fall within a tight band, then we are afforded some indication of the robustness of the computations.   


%Another reason frames might be a good idea is that high-frequency financial data is seldom without market microstructure noise, while Fourier and wavelet methods have noise reduction capabilities, Gabor frames are particularly efficient in this regards. As a result, Gabor frames can potentially yield much sparser representations of the volatility process when working in a noisy environment.  We will not deal explicitly with market microstructure noise here, we will do so in a second paper.  

Our discussion of  frame theory will be  rather brief; we only mention concepts needed for our specification of the volatility estimator.  For a  more detailed treatment see the book by \cite{Christensen2008}. 
In the sequel if $z$ is a complex number then we shall denote respectively by $\bar{z}$ and $\vert z \vert$ the complex conjugate and magnitude of $z$. Let \LtwoR denote the space of complex-valued functions defined on the real line with finite norm given by 
\begin{align}
  \Vert f \Vert := \left(\int_\real f(t) \overline{f(t)} \D t\right)^{1/2} < \infty, \qquad  f \in \LtwoR.\notag
  \label{}
\end{align}
 Define the  inner product of two elements $f$ and $g$ in $\LtwoR$ as $\langle f,g\rangle :=  \int_\real f(t) \overline{g(t)} \D t$.

 Denote by \ltwo the set of  complex-valued sequences defined on the set of natural numbers \nats with finite norm given by 
 \begin{align}
   \Vert c \Vert := \left( \sum_{k \in \nats} c_k \overline{c_k} \right)^{1/2} <   \infty, \qquad c \in \ltwo,\notag
   \label{}
 \end{align}
 where $c_k$ is the $k$-th component of $c$. The inner product of two sequences $c$ and $d$ in \ltwo is $\langle c, d \rangle := \sum_{k \in \nats} c_k \overline{d_k}$. Now we may give a definition for frames:
\begin{defn}\label{eq:frbound}
   A sequence $\{f_k\}_{k \in \nats} \subset \LtwoR$ is a frame if there exists positive constants $C_1$ and $C_2$ such that
  \begin{align}
    C_1\Vert f \Vert^2 \le \sum_{k \in \nats}\vert \langle f, f_k\rangle \vert^2 \le  C_2 \Vert f \Vert^2,
  \qquad   f \in \LtwoR. \notag 
  \end{align}
\end{defn}
\noindent The  constants $C_1$ and $C_2$ are called   \emph{frame bounds}. If $C_1 = C_2$ then $\{f_k \}_{k \in \nats}$ is said to be \emph{tight}. Because an  orthonormal basis satisfies  Parseval's equality\footnote{
  Parseval's equality states that if $\{f_k\}_{k \in \nats}$ is an orthonormal basis for $\mcal{H}$, a separable Hilbert space, then
  \begin{align}
    \Vert f \Vert^2 = \sum_{k \in \nats} \vert \langle f, f_k \rangle \vert^2 =\Vert \hat{f} \Vert^2, \qquad  f \in \mcal{H},  \notag
    \label{}
  \end{align}
  where $\hat{f}$ is the Fourier transform of $f$.
}, it follows that an orthonormal basis is a tight frame with frame bounds identically equal to 1, i.e. $C_1 = C_2 = 1$.  Now if $\{f_k\}$ is a frame, we may associate with it a bounded operator $\mcal{A}$ that maps every function  $f$ in  \LtwoR to a sequence $c$ in \ltwo in the following way:
\begin{align}
  &\mcal{A} f  = c \qquad \text{where} \qquad c_k = \langle f, f_k\rangle, \qquad  k \in \nats. 
  \label{eq:analysis}
\end{align}
Because $\mcal{A}$ takes a function defined on a continuum (\real) to a sequence, which is a function defined on the discrete set \nats, $\mcal{A}$ is known as the \emph{analysis} operator associated with the frame $\{f_k\}_{k \in \nats}$. The boundedness of the analysis operator follows from the frame bounds in  Definition \eqref{eq:frbound}. Now $\mcal{A}^*$, the adjoint   of $\mcal{A}$, is well-defined and takes sequences in \ltwo to functions in \LtwoR.  Using the fact that  $\mcal{A}^*$ must satisfy the equality $\langle \mcal{A} f  , c\rangle = \langle f,\mcal{A}^*c\rangle$ for all $f \in \LtwoR$ and $c \in \ltwo$, it may be deduced that
\begin{align}
  \mcal{A}^* c = \sum_{k \in \nats} c_k f_k, \qquad  c \in \ltwo,\notag
  \label{}
\end{align}
where $c_k$ is the $k$-th component of the sequence $c$. The adjoint, $\mcal{A}^*$, may be thought of as reversing the operation or effect of the analysis operator; for this reason it is known as the \emph{synthesis} operator.
\begin{comment}
By composing the analysis and the synthesis operators, we obtain the \emph{frame operator} $F:\LtwoR \to \LtwoR$ defined as: 
\begin{align}
  \mcal{F} f := \mcal{A}\mcal{A}^*f = \sum_{k \in \nats} \langle f, f_k \rangle f_k, \notag
\end{align}
for all $f \in \LtwoR$.
The frame operator $F$ is bounded, invertible, and self-adjoint\footnote{See \cite{Christensen2001} and the references therein.}. This yields the representation result
\begin{align}
  f = FF^{-1}f = \sumn \langle f, F^{-1} f_k\rangle f_k. \notag 
  \label{}
\end{align}
The sequence $\{F^{-1}f_k\}_{k \in \nats}$ is also a frame, and it is called the \emph{canonical dual} of $\{f_k\}_{k \in \nats}$. A frame will generally have other duals besides the canonical dual. That is, there exists  sequences $\{\tilde{f}_k\}_{k \in \nats}$ besides the canonical sequence such that 
\begin{align}
  f = \sumn \langle f, \tilde{f}_k\rangle f_k \qquad  f \in \LtwoR.
  \label{eq:dual}
\end{align}
\end{comment}


Now an application of the operator $(\mcal{A}^*\mcal{A})^{-1}$ to every frame element $f_k$ yields a sequence $\{\tilde{f}_k := (\mcal{A}^*\mcal{A})^{-1}f_k\}_{k \in \nats}$, which  is yet another frame for $\LtwoR$. The frame $\{\tilde{f}_k\}_{k \in \nats}$ is known as the \emph{canonical dual} of $\{f_k\}_{k \in \nats}$. Denoting the  analysis operator associated with the canonical dual by $\tilde{\mcal{A}}$, it  may be shown\footnote{See for example \citet[Proposition 3.2.3]{Daubechies1992}} that 
\begin{align}
  \mcal{A}^* \tilde{\mcal{A}}= \tilde{\mcal{A}}^* \mcal{A} = \mcal{I},\label{eq:frep}
\end{align}
where $\mcal{I}$ is the  identity operator and $\tilde{\mcal{A}}^*$ is the adjoint of the  analysis operator of the canonical dual. Furthermore, Proposition 3.2.3 of \cite{Daubechies1992} shows that   $\tilde{\mcal{A}}$ satisfies 
\begin{align}
  \tilde{\mcal{A}} = \ap (\ap^*\ap)^{-1},
  \label{eq:charac}
\end{align}
so that the analysis operator of the canonical dual frame is fully characterized by \ap and its adjoint. It is easily seen that \eqref{eq:frep} yields a representation result since if $f \in \LtwoR$ then  
\begin{align}
  f =  \tilde{\ap}^*\ap f = \mcal{A}^* \tilde{\mcal{A}}f = \sum_{k \in \nats}\langle f, \tilde{f}_k\rangle f_k.  
  \label{eq:frepresent}
\end{align}
Thus, in a manner reminiscent of orthonormal basis representations, every function in \LtwoR is expressible as a linear combination of the frame elements, with the frame coefficients given by  $\langle f, \tilde{f}_k\rangle$, the correlation between the function and the elements of the dual frame. It follows from the first equality in \eqref{eq:frep} and the commutativity of the duality relationship that functions in \LtwoR may also be written as linear combinations of the elements in $\{\tilde{f}_k\}_{k \in \nats}$, with coefficients given by $\langle f, {f}_k\rangle$, i.e. $f = \sum_{k \in \nats} \langle f, {f}_k\rangle \tilde{f}_k$.


Now let $\mcal{P}:= \tilde{\mcal{A}}\mcal{A}^*  = \mcal{A}\tilde{\mcal{A}}^*$. A consequence of the noncommutativity\begin{comment}\footnote{A binary operation $\star$ is noncommutative if $A\star B$ is generally not equal to $B\star A$. A classic  example is matrix multiplication.}\end{comment} of the composition operator is that even though $\mcal{A}^*\tilde{\mcal{A}} = \mcal{I}$  \eqref{eq:frep}, the operator $\mcal{P}$   need not be equal to $\mcal{I}$. In fact, Proposition 3.2.3 in \citep{Daubechies1992} shows that in the general case $\mcal{P}$ is the orthogonal projection operator of \ltwo onto $R(\ap)$, the range space of $\mcal{A}$, that is, {$R(\mcal{A}) := \{c \in \ltwo : c = \mcal{A} f, \text{ for some } f \in \LtwoR\}$}.   That $R(\mcal{A})$ in general may not coincide  with \ltwo is a consequence of the fact that  frames  may be redundant, i.e.  that they may contain ``more'' elements than is required, and thus may be linearly dependent. The level of redundancy is inversely related to the ``size'' of  $R(\mcal{A})$. So that the more redundancy the frame contains the smaller the range space of \ap. To see this, let $\{f_k\}_{k \in \nats}$ be a  frame  such that $f_1 = f_2 + f_3$; so, $\{f_k\}_{k \in \nats}$ is redundant and  linearly dependent. Now if $c$ is in the range space of \ap, the associated analysis operator, then  the $k$-th coordinate of $c$ must satisfy $c_k = \inner{f}{f_k}$  for some $f \in \LtwoR$.  By the linearity of the inner product, $c_1 = c_2 + c_3$. Thus every element of $R(\ap)$ must satisfy the restriction that its first component must equal the sum of the second and the third. Since not every element of  \ltwo is subject to this restriction, $R(\ap)$ must be a proper subset of \ltwo. So, dependence relationships among the frame elements serve to restrict the range of \ap; the greater the number of dependence relationships there are, the smaller $R(\ap)$ becomes.   As we shall see shortly, this fact has important consequences for how coefficient error affects the precision of the synthesis operator. Of course if the frame has no redundancy, then $R(\mcal{A})$   will coincide with \ltwo and $\mcal{P}$ will just be the identity operator. 
\subsection{Coefficient noise reduction}\label{sub:why}
The main reason we might be interested in frame methods for estimating volatility is robustness to coefficient noise. By this we mean the imprecision that may result by virtue of the fact that in practice the frame coefficients may not be known with precision and must be estimated. Coefficient error has many sources: error resulting from using a finite data sample, rounding or quantization error,  and error arising from the use of data contaminated with market microstructure noise. 


The robustness of redundant frames to coefficient error is well-documented. For instance, \cite{Munch1992} report noise reduction that is directly proportional to the degree of redundancy of the frame. \cite{Cvetkovic1998} consider coefficient error due to quantization, and report an even high degree of robustness to this type of coefficient errors. That redundant frames exhibit this kind of robustness is not entirely unexpected. Redundant frames in essence include near-duplicates of frame elements; so that, any error arising from a given frame coefficient is easily made up for by the presence of other frame elements with similar informational content. 


\cite{Daubechies1992} provides the following heuristic explanation in terms of the size of the range space of the analysis operator. Let $\{f_k\}_{k \in \nats}$ be a redundant  frame in \LtwoR, and $\mcal{A} : \LtwoR \to \ltwo$ be the associated analysis operator defined as in \eqref{eq:analysis}.  Now, it follows from   \eqref{eq:frep} and \eqref{eq:charac} that   
\begin{align}
  \mcal{I} = \tilde{\ap}^*\ap  = \tilde{\ap}^*\tilde{\ap}\ap^*\ap. \notag
  \label{}
\end{align}
From the discussion in the previous section,   the orthogonal projector of \ltwo onto $R(\mcal{A})$  is  $\tilde{\mcal{A}}\mcal{A}^* = \mcal{P}$. Combining this with the equation above, we have $\mcal{I} =  \tilde{\mcal{A}}^* \mcal{P} {\mcal{A}}$. So the representation result in \eqref{eq:frepresent} can be expressed as 
\begin{align}
  f = \tilde{\ap}^* \mcal{P} \ap f.\notag
  \label{}
\end{align}
Now assuming the coefficients of $f$ under the operation of the analysis operator were contaminated by white noise sequence $\varepsilon$, we would have at our disposal ${\ap}f + \varepsilon$ instead of simply ${\ap} f$. Further assume that $\varepsilon$ is decomposable as  follows:  $\varepsilon = \varepsilon_{\mcal{A}^\perp}+ \varepsilon_A$, where $\varepsilon_A$ resides in the range space of \ap, and  $\varepsilon_{\mcal{A}^\perp}$ resides in the orthogonal complement  of $R(\ap)$. So, by definition, $\mcal{P} \varepsilon_{\mcal{A}^\perp} = 0$.  The operation of reconstructing a function from the noisy coefficients may now be expressed as 
\begin{align}
  f_\varepsilon = \tilde{\mcal{A}}^*\mcal{P}({\mcal{A}}f + \varepsilon) = \tilde{\mcal{A}}^*\mcal{P}({\mcal{A}}f + \varepsilon_A + \varepsilon_{\mcal{A}^\perp}) = \tilde{\mcal{A}}^*\mcal{P}({\mcal{A}}f + \varepsilon_A ). \notag
  \label{}
\end{align}
It is thus clear  that the deviation of the approximation, $\Vert f - f_\varepsilon \Vert$, equals  $\Vert\tilde{\ap}^*\mcal{P}\varepsilon_A \Vert$, which  should be lower than $\Vert \varepsilon \Vert$ to the extent that the range space of $\mcal{A}$ is small; this is another way of saying that the approximation error is reduced to the extent that the frame is redundant. As noted by \citet[pp. 98]{Daubechies1992}, this explanation is heuristic and probably accounts for only a small portion of the noise reduction  observed in practical work. Nevertheless,  it provides a starting point for thinking about the source of the robustness of frames. 
\subsection{ Gabor frames}
Next, we specialize the discussion to Gabor frames. The analysis of Gabor frames involves two operators: the \emph{translation} operator  $\mcal{T}$ and  the \emph{modulation} operator $\mcal{M}$ defined as follows: 
\begin{align}
  & \mcal{T}_bf(t) := f(t -  b), &\qquad b \in \real, f \in \LtwoR, \label{eq:translation}\\
  & \mcal{M}_af(t) := \mathrm{e}^{2\pi \i at}f(t), &\qquad a \in \real, f \in \LtwoR,\label{eq:modulation} 
\end{align}
where $\i$ is the imaginary number, i.e.  $\i = \sqrt{-1}$.  Both $\mcal{T}$ and $\mcal{M}$ are shift operators: $\mcal{T}$ is a shift or translation operator on the time axis, whereas $\mcal{M}$ performs shifts on the frequency axis. A Gabor system is constructed by fixing $a, b \in \real$, and  performing shifts of a single nontrivial  function $g \in \LtwoR$  in time-frequency space. For example,  if $a$ and $b$ are real numbers then the sequence of functions  
\begin{align}
  \{\mcal{M}_{ha} \mcal{T}_{kb} g\}_{h,k \in \ints},\notag
  \label{}
\end{align}
constitutes  a Gabor system. 
\begin{defn}\label{defn:def}
  Let $g \in \LtwoR$, and let  $a > 0$, $b > 0$ be  positive real numbers. Define for $t \in \real$    
\begin{align}
  g_{h,k}(t) := \mathrm{e}^{2 \pi \i h a t }g(t - k b), \qquad   h,k \in \ints. \notag    
 \end{align}
 If the sequence $\{g_{h,k}\}_{h,k \in \ints}$ constitutes a  frame for \LtwoR, then it is called a Gabor frame.\footnote{It is also sometimes referred to as a \emph{Weyl-Heisenberg} frame.} 
\end{defn}
\noindent The fixed function $g$ is known as  the \emph{Gabor frame generator}\footnote{It is  referred to elsewhere as the \emph{window function}.}; $a$ is known as the \emph{modulation parameter}; and $b$ is known as the \emph{translation parameter}.   
In order to obtain sharp asymptotic rates, we require $g$ and its dual \tg (see \eqref{eq:frepresent}) to be continuous and compactly supported. The following result, stated  in  \citet[Lemma 1.2]{Christensen2006} and in  \citet[Proposition 2.4]{Zhang2008}, tells us how to construct such dual pairs.
\begin{lem}\label{le:gabor}
  Let $[r,s]$ be a finite interval,  let $a> 0$, $b > 0$  be positive constants,  and let $g$ be a continuous function. If $g(t) \ne 0$ when $t \in (r,s)$; $g(t) = 0$ when $t \notin (r,s)$;  and  $a$, $b$ satisfy: $a < 1/(s-r)$, $0<b<s-r$; then  $\{g,\tilde{g}\}$ is a pair of dual Gabor frame generators, with the dual Gabor generator given by  
\begin{align}
  &\tilde{g}(t)  :=  g(t)/G(t), \text{ where} \label{eq:dualg}\\
  &G(t) := \sumi|g(t-kb)|^2/a.\label{eq:capg}
\end{align}
Furthermore, 
\begin{align}
  \tilde{g}_{h,k}(t) := \mathrm{e}^{2\pi\i h a t} \tilde{g}(t - kb), \qquad  h, k \in \ints \label{eq:dualghk}
\end{align}
is compactly supported.   
\end{lem}
%\begin{comment}
\noindent Next, we state here and prove in the Appendix that the dual generator \tg  inherits the continuity properties of $g$.
\begin{lem} \label{lem:modtg}
  Let the dual Gabor frame generator $\tilde{g}$ be constructed as in \eqref{eq:dualg}. If $\modc{g}{\delta}$ denotes the modulus of continuity of $g$, i.e. $\modc{g}{\delta} := \sup \{|g(t) - g(t')| : t,t' \in \real \text{ and } |t -t'| < \delta\}$,  then   
  \begin{align}
    \modc{\tilde{g}_{j,k}}{\delta} \le C \modc{g}{\delta} \qquad  \hkints\notag,
    \label{}
  \end{align}
  where $C$ is a positive constant.
\end{lem}
\begin{proof}
  See Appendix \ref{ap:proof}.
\end{proof}
%\end{comment}
\noindent In the sequel,  we assume the Gabor frame setup in Lemma \ref{le:gabor}.
\begin{comment}
Frames are quite general objects. What is needed is some control over the type of redundancies allowed in a frame. Without such a restriction results about the rate of convergence of the frame expansion would be impossible to come by. A Riesz basis provides just the type of control needed. Informally, a Riesz basis is a frame whose elements are all essential. 
\begin{defn}
  A sequence $\{f_k\}_{k \in \nats}$, with $f_k \in \LtwoR$ for all $k$, is a Riesz basis  if there exists an orthonormal basis $\{\xi_k\}_{k \in \nats}$ of \LtwoR and  a bounded invertible operator $\mcal{T}: \LtwoR \to \LtwoR$ such that $f_k = T \xi_k$, for all $k$. 
\end{defn}
\noindent A frame is Riesz basis if it is \emph{complete}; i.e. whenever $\langle f,f_k\rangle = 0$ for all $k$ then $f =0$; and there are positive constants $c,C$ such that 
\begin{align}
  c\sum_{k=1}^N\vert c_k\vert^2 \le \left\Vert \sum_{k =1}^N c_k f_k \right \Vert^2 \le C\sum_{k=1}^N\vert c_k\vert^2,
  \label{}
\end{align}
for all finite sequences $\{c_k\}_{1\le k\le N}$. This is equivalent to the condition
\begin{align}
  c\le \sumi \vert\hat{f}(\omega + 2 \pi k)\vert \le C, \qquad \forall\omega \in [0,2\pi], 
  \label{}
\end{align}
where $\hat{f}$ is the Fourier transform of $f$. 
To proceed in our analysis, we specialize further the type of Riesz basis to those that may be generated by a single element (function), $f \in \Ltwo$.  The entire Riesz basis is then generated by translating $f$ across the closed unit interval. By appropriately scaling the $f$ we end up with different levels of granularity in the representation. That is, we have in mind a collection\footnote{See \cite{Unser1997} for further elaboration on these ideas.} $\{f_{h,k}\}_{k, h \in \ints}$, where $f_{h,k} :=  f(x/h - k)$. We denote the function space generated by this basis as follows:
\begin{align}
  V_h(f) := \left\{\sumi c_{h,k} f_{h,k} : \{c_{h,k}\} \in \ltwo\right\}
  \label{}
\end{align}
\end{comment}
\input{volatility}
\input{empirics}
%\input{ext}
\begin{comment}
\section{Conclusion} \label{sec:conclusion}
We proposed an estimator for the spot volatility function using Gabor frame methods. We showed that the estimator converges in a MISE sense and obtained an explicit convergence rate.  The evidence for the validity of the proposed estimator will be further reinforced in a simulation study. We will also take the estimator to task using data from the Forex and bond market. 
\end{comment}
%\input{app}
\input{conclusion}
\pagebreak
\input{semiapp}
\input{/home/wale/Dropbox/PhD/Bibliography/masterbiblinux}
\end{document}

